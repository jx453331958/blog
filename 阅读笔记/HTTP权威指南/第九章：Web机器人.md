# HTTP 权威指南

## 第 9 章 Web 机器人

1. Web 机器人是能够在无需人类干预的情况下自动进行一系列 Web 事务处理的软件程序。

2. Web 爬虫是一种机器人，它们会递归地对各种信息性 Web 站点进行遍历，获取第一个 Web 页面，然后获取那个页面指向的所有 Web 页面。递归地追踪这些 Web 链接的机器人会沿着 HTML 超链接创建的网络“爬行”，所以将其称为爬虫或蜘蛛。

3. 根集：爬虫开始访问的 URL 初始集合。

4. 环路对爬虫是有害的：

    1. 它们会使爬虫陷入可能会将其困住的循环之中。循环会使未经良好设计的爬虫不停地兜圈子，把所有时间都耗费在不停地获取相同的页面上。爬虫会消耗很多的带宽，可能完全无法获取任何其他页面了。
    2. 爬虫不断地获取相同的页面时，会对另一端的服务器造成压力。
    3. 爬虫重复获取相同的内容会使这一应用程序毫无用处。

5. 大规模爬虫对其访问过的地址进行管理时使用的技术：

    1. 树和散列表：复杂的机器人可能会用搜索树或散列表来记录已访问的 URL。这些是加速 URL 查找的软件数据结构。
    2. 有损的存在位图：为了减小空间，一些大型爬虫会使用有损数据结构，比如存在位数组。用一个散列函数将每个 URL 都转换成一个定长的数字，这个数字在数组中有个相关的“存在位”。爬行过一个 URL 时，就将相应的“存在位”置位。如果存在位已经置位了，爬虫就认为已经爬行过那个 URL 了。
    3. 检查点：将已访问的 URL 列表保存到硬盘上，以防机器人程序崩溃。
    4. 分类：使用机器人集群，以汇接方式工作。为每个机器人分配一个特定的 URL，所有机器人配合工作，爬行整个 Web。

6. 避免循环和重复：实际上，经过良好设计的机器人中要包含一组试探方法，以避免环路的出现。

7. 爬虫机器人常用的技术：
    1. 规范化 URL：将 URL 转换为标准形式以避免语法上的别名
    2. 广度优先的爬行：以广度优先的方式来调度 URL 去访问 Web 站点，就可以将环路的影响最小化
    3. 节流：限制一段时间内机器人可以从一个站点获取的页面数量。如果机器人跳入了一个环路，试图不断的访问某个站点的别名，也可以通过节流来限制重复的页面总数和对服务器的访问次数。
    4. 限制 URL 的大小：机器人可能会拒绝爬行超出特定长度的 URL。如果环路使 URL 的长度增加，长度限制就会最终终止这个环路。
    5. URL/站点黑名单：维护一个与机器人环路和陷阱相对应的已知站点及 URL 列表，然后不进行爬取。发现新问题时，就将其加入黑名单。
    6. 模式检测：文件系统的符号连接和类似的错误配置所造成的环路会遵循某种模式，比如URL会随着组件的复制逐渐增加。有些机器人会将具有重复组件的URL当作潜在的环路，拒绝爬行带有多余两个或三个重复组件的URL。
    7. 内容指纹：使用内容指纹的机器人会获取页面内容中的字节，并计算出一个校验和。这个校验和是页面内容的压缩表现形式。爬取新页面时可以进行比对。MD5
    8. 
